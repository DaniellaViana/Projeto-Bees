# ğŸº Projeto AB InBev â€“ Pipeline de Dados - Cervejarias

## ğŸ‘©â€ğŸ’» Autora

Daniella Viana  
[github.com/DaniellaViana](https://github.com/DaniellaViana) 

## ğŸ“Œ Objetivo
Este projeto implementa um pipeline de dados ETL (Extract, Transform, Load), utilizando o **Apache Airflow** para orquestraÃ§Ã£o, e **Pandas** para processamento de dados. A arquitetura segue o modelo **Lakehouse**, com as camadas Bronze, Silver e Gold, permitindo fÃ¡cil rastreabilidade e modularidade.

## ğŸ“ Estrutura do Projeto

A estrutura do repositÃ³rio estÃ¡ organizada da seguinte forma:

Projeto-Bees/ <br>â”‚ â”œâ”€â”€ dags/ # CÃ³digo principal
<br>â”‚    â””â”€â”€ brewery_pipeline.py
<br>â”‚ â”œâ”€â”€ data/ # Camadas do Lakehouse
<br>â”‚ â”œâ”€â”€ bronze/ # Dados brutos extraÃ­dos 
<br>â”‚ â”œâ”€â”€ silver/ # Dados limpos/tratados
<br>â”‚ â””â”€â”€ gold/ # Dados agregados para anÃ¡lise 
<br>â”‚ â”œâ”€â”€ tests/ # Testes de extraÃ§Ã£o e transformaÃ§Ã£o
<br>â”‚ â”œâ”€â”€ extract_test.py
<br>â”‚ â””â”€â”€ transform_test.py 
<br>â”‚ â”œâ”€â”€ docker-compose.yml # ConfiguraÃ§Ã£o do Docker para o Airflow 
<br>â””â”€â”€ .gitignore # Arquivo de configuraÃ§Ã£o do Git
 
## â–¶ï¸ ExecuÃ§Ã£o

### 1. Clone o repositÃ³rio em seu ambiente local:
   
```git clone https://github.com/DaniellaViana/Projeto-Bees.git```
<br><br>cd Projeto-Bees

### 2. Subir o ambiente com Airflow

Este projeto jÃ¡ vem com um docker-compose.yml pronto para rodar o Airflow localmente:

**Execute o comando abaixo:**

```docker-compose up --build```

Acesse a interface do Airflow em:
ğŸ‘‰ http://localhost:8080

**Login:**

UsuÃ¡rio: admin
<br>Senha: admin

### 3. Executar o pipeline
Na interface do Airflow, ative a DAG **brewery_data_pipeline** (se nÃ£o estiver ativa). VocÃª pode executÃ¡-la manualmente ou aguardar o agendamento automÃ¡tico (Todos os dias Ã s 22h).

### 4. Resultado da execuÃ§Ã£o
ApÃ³s a execuÃ§Ã£o, os arquivos de dados transformados estarÃ£o disponÃ­veis nas seguintes localizaÃ§Ãµes:

**data/silver/breweries.parquet**

**data/gold/brewery_summary.parquet**

## âš™ï¸ Tecnologias e Arquitetura

**Apache Airflow:** OrquestraÃ§Ã£o das tarefas de ETL, proporcionando monitoramento e escalabilidade futura.

**Pandas:** Processamento de dados tabulares, ideal para trabalhar com grandes volumes de dados em Python.

**Arquitetura Lakehouse:** A separaÃ§Ã£o dos dados em Bronze, Silver e Gold facilita a rastreabilidade, modularidade e governanÃ§a de dados.

**Bronze:** Dados brutos extraÃ­dos de fontes externas.

**Silver:** Dados limpos e tratados, prontos para anÃ¡lise.

**Gold:** Dados agregados para visualizaÃ§Ãµes e insights.

O pipeline foi modularizado, com funÃ§Ãµes especÃ­ficas para cada etapa:

ExtraÃ§Ã£o de dados.
TransformaÃ§Ã£o dos dados (limpeza, formataÃ§Ã£o, etc).
AgregaÃ§Ã£o para anÃ¡lise e visualizaÃ§Ã£o.

# ğŸ”„ Trade-offs
**Uso do LocalExecutor com PostgreSQL:** o projeto utiliza o Airflow com LocalExecutor e PostgreSQL como metadatabase, o que oferece maior robustez e paralelismo local. Ainda assim, o LocalExecutor nÃ£o oferece a mesma escalabilidade que opÃ§Ãµes como KubernetesExecutor.

**Processamento em memÃ³ria com Pandas:** ideal para datasets pequenos e prototipagem rÃ¡pida. 

**Fonte de dados externa:** a ingestÃ£o de dados Ã© feita via requisiÃ§Ã£o GET a uma API externa, o que simula com fidelidade um cenÃ¡rio real de extraÃ§Ã£o dinÃ¢mica.

**OrquestraÃ§Ã£o baseada em tempo:** a DAG Ã© executada com base em agendamento temporal. EstÃ¡ agendada para executar todos os dias Ã s 22h.

# ğŸ› ï¸ Testes
O projeto inclui testes automatizados para garantir que as etapas de extraÃ§Ã£o e transformaÃ§Ã£o estejam funcionando corretamente:

**Testes de extraÃ§Ã£o:** Localizados em tests/extract_test.py.

**Testes de transformaÃ§Ã£o:** Localizados em tests/transform_test.py.

Executar:

```docker exec -it projetobees-airflow-webserver-1 /bin/bash```

Instalar o PYTEST

```pip install pytest```

Executar os testes:

```pytest tests/extract_test.py```

```pytest tests/transform_test.py```

# âš ï¸ Monitoramento/Alertas

**Foram incluÃ­dos os seguintes monitoramentos:**

Logs detalhados nas tasks (extract, transform, aggregate) usando logging do Python para registrar o que foi feito em cada etapa.

Retries e timeout configurados em cada tarefa da DAG, para evitar falhas silenciosas.

Alertas automÃ¡ticos por e-mail em caso de falha de qualquer tarefa no Airflow:

Exemplo:

```
default_args = {
    'owner': 'airflow',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email': ['colocar o email aqui'],
}
```
